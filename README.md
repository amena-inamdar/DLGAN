# DLGAN
Basic GAN Implementation

Description for a GAN (Generative Adversarial Network) implementation for image generation using TensorFlow. Here's a summary of the key components and steps involved in this GAN implementation:
1. Introduction to GANs:
   - GANs, or Generative Adversarial Networks, consist of two neural networks: a discriminator and a generator.
   - The discriminator is trained to classify images as real (from the training set) or fake (generated by the generator).
   - The generator takes random noise as input and produces images, aiming to fool the discriminator into classifying them as real.

2. Minimax Game:
   - GAN training can be seen as a minimax game where the generator minimizes the probability of the discriminator making the correct choice, while the discriminator maximizes the probability of making the correct choice on both real and generated data.

3. Alternating Updates:
   - Training alternates between updating the generator to maximize the discriminator's incorrect choices and updating the discriminator to maximize the correct choices.

4. LeakyReLU Activation:
   - The LeakyReLU activation function is used in the discriminator to prevent units from "dying" during training.

5. Random Noise Generation:
   - Random noise is generated with values in the range [-1, 1] to serve as input to the generator.

6. Discriminator Architecture:
   - The discriminator consists of fully connected layers with LeakyReLU activations.
   - Architecture:
     - Fully connected layer (784 input, 256 output) with LeakyReLU (alpha 0.01)
     - Fully connected layer (256 output) with LeakyReLU (alpha 0.01)
     - Fully connected layer (1 output)

7. Generator Architecture:
   - The generator also consists of fully connected layers with ReLU activations.
   - Architecture:
     - Fully connected layer (input size varies with noise dimensions, output size 1024) with ReLU
     - Fully connected layer (1024 output) with ReLU
     - Fully connected layer (784 output) with TanH (output constrained to [-1, 1])

8. GAN Loss:
   - The generator loss is computed using the negative log probability of the discriminator making the correct choice for generated data.
   - The discriminator loss is computed using the negative log probability of the discriminator making the correct choice for both real and generated data.

9. Optimizer:
   - Adam optimizer is used with a learning rate of 1e-3 and beta1=0.5 for minimizing both the generator and discriminator losses.

10. Training
   - Training proceeds with one batch of real data and one batch of generated data for each iteration.
   - The generator and discriminator losses are minimized iteratively during training.

11. Visualization:
   - Over epochs, the generated images become progressively better, with clearer shapes and details.

This GAN implementation aims to generate images that resemble the training data (in this case, MNIST digits) by training a generator to create realistic images and a discriminator to distinguish between real and fake images. The process of alternating updates between the generator and discriminator helps both networks improve their performance over time.
